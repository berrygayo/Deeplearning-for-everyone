{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 로지스틱 회귀의 정의\n",
    "로지스틱 회귀는 선형 회귀와 마찬가지로 적절한 선을 그려가는 과정입니다.\n",
    "다만, 직선이 아니라, 참(1)과 거짓(0) 사이를 구분하는 s자 형태의 선을 그어주는 작업입니다.\n",
    "![nn](5.1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시그모이드 함수\n",
    "이러한 S자 형태로 그래프가 그려지는 함수가 시그모이드 함수(sigmoid function)입니다.\n",
    "e는 상수로 고정되어있고, 우리가 따로 구해야 하는 값은 ax+b입니다.\n",
    "\n",
    "![nn](5.2.jpg)\n",
    "\n",
    "a는 그래프의 경사도를 결정합니다. a값이 커지면 경사가 커지고 작아지면 경사가 작아집니다. \n",
    "![nn](5.3.jpg)\n",
    "\n",
    "b는 그래프의 좌우 이동을 의미합니다. \n",
    "![nn](5.4.jpg)\n",
    "\n",
    "a값이 크고 작아짐에 따라 오차가 다음과 같이 변합니다.\n",
    "a값이 작아지면 오차는 무한대로 커집니다. 그런데 a 값이 커진다고 해서 오차가 무한대로 커지지는 않습니다. \n",
    "\n",
    "![nn](5.5.jpg)\n",
    "\n",
    "b값은 너무 크거나 작을 경우 오차가 무한대로 커지므로 앞서와 마찬가지로 이차 함수 그래프로 표현할 수 있습니다.\n",
    "![nn](5.6.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오차 공식\n",
    "a와 b의 값을 어떻게 구해야 할까요? 답은 역시 '경사 하강법'입니다. \n",
    "경사 하강법은 먼저 오차를 구한 다음 오차가 작은 쪽으로 이동시키는 방법이라고 했습니다. 그렇다면 이번에도 예측 값과 실제 값의 차이, 즉 오차를 구하는 공식이 필요합니다. \n",
    "\n",
    "시그모이드 함수의 특징은 y 값이 0과 1사이라는 것입니다. 따라서 실제 값이 1일 때 예측 값이 0에 가까워지면 오차가 커져야 합니다. 반대로, 실제 값이 0일때 예측 값이 1에 가까워지는 경우에도 오차는 커져야 합니다. 이를 공식으로 만들 수 있게 해 주는 함수가 바로 로그함수입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 로그 함수 \n",
    "\n",
    "아래와 같이 y=0.5에 대칭하는 두 개의 로그 함수를 그려보겠습니다. \n",
    "![nn](5.7.jpg)\n",
    "\n",
    "파란선은 실제 값이 1일 때 사용할 수 있는 그래프 입니다. 예측 값이 1일때 오차가 0이고, 반대로 예측 값이 0에 가까울수록 오차는 커집니다. \n",
    "빨간색 선은 반대로 실제 값이 0일 때 사용할 수 있는 함수입니다. 예측 값이 0일때 오차가 없고, 1에 가까워질수록 오차가 매우 커집니다. \n",
    "\n",
    "앞의 파란색과 빨간색 그래프의 식은 -logh 와 -log(1-h) 입니다.\n",
    "y의 실제 값이 1일 때 -logh 그래프를 쓰고, 0일 때 -log(1-h) 그래프를 써야 합니다.\n",
    "이는 다음과 같은 방법으로 해결할 수 있습니다. \n",
    "\n",
    "![nn](5.8.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 코딩으로 확인하는 로지스틱 회귀 \n",
    "\n",
    "이를 그대로 텐서플로로 옮겨 보겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "data=[[2,0],[4,0],[6,0],[8,1],[10,1],[12,1],[14,1]] #공부시간과 합격 여부 \n",
    "x_data=[x_row[0] for x_row in data]\n",
    "y_data=[y_row[1] for y_row in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a와 b의 값을 임의로 정합니다.\n",
    "a=tf.Variable(tf.random_normal([1],dtype=tf.float64,seed=0))\n",
    "b=tf.Variable(tf.random_normal([1],dtype=tf.float64,seed=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#시그모이드 함수 방정식을 넘파이 라이브러리를 이용해 다음과같이 작성합니다. \n",
    "\n",
    "y=1/(1+np.e**(-a*x_data+b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nn](5.9.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#오차 \n",
    "\n",
    "loss=-tf.reduce_mean(np.array(y_data)*tf.log(y)+(1-np.array(y_data))*tf.log(1-y))\n",
    "#오차를 구하고 그 평균값으 loss 변수에 할당합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0707 16:37:46.342468 13864 deprecation.py:323] From C:\\Users\\Berry\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "#학습률을 지정하고 경사 하강법을 이용해 오차를 최소로 하는 값을 찾습니다. \n",
    "learning_rate=0.5\n",
    "gradient_decent=tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss=4.4835, 기울기 a=2.3946, y절편=-0.8820\n",
      "Epoch: 6000, loss=0.0152, 기울기 a=2.9209, y절편=20.2966\n",
      "Epoch: 12000, loss=0.0081, 기울기 a=3.5636, y절편=24.8002\n",
      "Epoch: 18000, loss=0.0055, 기울기 a=3.9556, y절편=27.5458\n",
      "Epoch: 24000, loss=0.0041, 기울기 a=4.2380, y절편=29.5227\n",
      "Epoch: 30000, loss=0.0033, 기울기 a=4.4585, y절편=31.0672\n",
      "Epoch: 36000, loss=0.0028, 기울기 a=4.6395, y절편=32.3343\n",
      "Epoch: 42000, loss=0.0024, 기울기 a=4.7929, y절편=33.4084\n",
      "Epoch: 48000, loss=0.0021, 기울기 a=4.9261, y절편=34.3404\n",
      "Epoch: 54000, loss=0.0019, 기울기 a=5.0436, y절편=35.1634\n",
      "Epoch: 60000, loss=0.0017, 기울기 a=5.1489, y절편=35.9003\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(60001):\n",
    "        sess.run(gradient_decent)\n",
    "        if i % 6000==0:\n",
    "            print(\"Epoch: %.f, loss=%.4f, 기울기 a=%.4f, y절편=%.4f\" %(i,sess.run(loss), sess.run(a), sess.run(b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 여러 입력 값을 갖는 로지스틱 회귀&코딩으로 확인하는 다중 로지스틱 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=300, a1=0.7417, a2=-0.3860, b=-2.7831, loss=0.2458\n",
      "step=600, a1=0.7483, a2=-0.1390, b=-4.1615, loss=0.1794\n",
      "step=900, a1=0.6710, a2=0.1612, b=-5.1724, loss=0.1419\n",
      "step=1200, a1=0.5789, a2=0.4451, b=-5.9789, loss=0.1171\n",
      "step=1500, a1=0.4894, a2=0.7004, b=-6.6519, loss=0.0995\n",
      "step=1800, a1=0.4074, a2=0.9274, b=-7.2300, loss=0.0864\n",
      "step=2100, a1=0.3338, a2=1.1292, b=-7.7370, loss=0.0763\n",
      "step=2400, a1=0.2681, a2=1.3092, b=-8.1886, loss=0.0683\n",
      "step=2700, a1=0.2096, a2=1.4708, b=-8.5958, loss=0.0617\n",
      "step=3000, a1=0.1572, a2=1.6168, b=-8.9667, loss=0.0564\n",
      "공부한 시간: 7, 과외 수업 횟수: 6\n",
      "합격 가능성:  86.23 %\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    " \n",
    "# 실행할 때마다 같은 결과를 출력하기 위한 seed 값 설정\n",
    "seed = 0\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(seed)\n",
    " \n",
    "# x, y의 데이터 값\n",
    "x_data = np.array([[2, 3,], [4, 3], [6, 4], [8, 6], [10, 7], [12, 8], [14, 9]])\n",
    "y_data = np.array([0, 0, 0, 1, 1, 1, 1]).reshape(7, 1)\n",
    " \n",
    "# 입력 값을 플레이스 홀더에 저장\n",
    "X = tf.placeholder(tf.float64, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float64, shape=[None, 1])\n",
    " \n",
    "# 기울기 a와 바이어스 b의 값을 임의로 정함\n",
    "a = tf.Variable(tf.random_uniform([2, 1], dtype=tf.float64))\n",
    "# [2, 1] 의미 : 들어오는 값은 2개, 나가는 값은 1개\n",
    "b = tf.Variable(tf.random_normal([1], dtype=tf.float64, seed=0))\n",
    " \n",
    "# y 시그모이드 함수의 방정식을 세운다.\n",
    "\n",
    "#a1x1+a2x2는 행렬곱을 이용ㅎ ㅐ[a1,a2]*[x1,x2]로도 표혈할 수 있습니다. \n",
    "#텐서플로에서는 matmul() 함수를 이용해 행렬곱을 적용합니다.\n",
    "#y=tf.sigmoid(tf.matmul(X,a)+b)\n",
    "\n",
    "y = tf.sigmoid(tf.matmul(X, a) + b)\n",
    " \n",
    "# loss를 구하는 함수\n",
    "loss = -tf.reduce_mean(Y * tf.log(y) + (1 - Y) * tf.log(1 - y))\n",
    " \n",
    "# 학습률 값\n",
    "learning_rate = 0.1\n",
    " \n",
    "# loss를 최소로 하는 값 찾기\n",
    "gradient_decent = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss);\n",
    " \n",
    "predicted = tf.cast(y > 0.5, dtype=tf.float64)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float64))\n",
    " \n",
    "# 학습\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    " \n",
    "    for i in range(3001):\n",
    "        a_, b_, loss_, _ = sess.run([a, b, loss, gradient_decent], feed_dict={X: x_data, Y: y_data})\n",
    "        if (i+1) % 300 == 0:\n",
    "            print(\"step=%d, a1=%.4f, a2=%.4f, b=%.4f, loss=%.4f\" % (i + 1, a_[0], a_[1], b_, loss_))\n",
    "    \n",
    "    #실제값 적용하기 \n",
    "    new_x = np.array([7, 6]).reshape(1, 2) # [7, 6]은 각각 공부한 시간과 과외 수업 횟수\n",
    "    new_y = sess.run(y, feed_dict={X: new_x})\n",
    " \n",
    "    print(\"공부한 시간: %d, 과외 수업 횟수: %d\" % (new_x[:, 0], new_x[:, 1]))\n",
    "    print(\"합격 가능성: %6.2f %%\" % (new_y*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
