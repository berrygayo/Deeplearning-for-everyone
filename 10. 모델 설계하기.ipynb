{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2장 \n",
    "## 폐암 수술 환자의 생존율 예측하기\n",
    "### 코드 가져옴 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#딥러닝을 구동하는 데 필요한 케라스 함수를 불러옵니다. \n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense \n",
    "\n",
    "#필요한 라이브러리를 불러옵니다.\n",
    "import numpy\n",
    "import tensorflow as tf \n",
    "\n",
    "#실행할 때마다 같은 결과를 출력하기 위해 설정하는 부분입니다.\n",
    "seed=0\n",
    "numpy.random.seed(seed)\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#준비된 수술 환자 데이터를 불러옵니다.\n",
    "Data_set=numpy.loadtxt(\"C:/Users/Berry/Documents/Bigdata_study/deeplearning/dataset/ThoraricSurgery.csv\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[293.  ,   1.  ,   3.8 , ...,   0.  ,  62.  ,   0.  ],\n",
       "       [  1.  ,   2.  ,   2.88, ...,   0.  ,  60.  ,   0.  ],\n",
       "       [  8.  ,   2.  ,   3.19, ...,   0.  ,  66.  ,   1.  ],\n",
       "       ...,\n",
       "       [406.  ,   6.  ,   5.36, ...,   0.  ,  62.  ,   0.  ],\n",
       "       [ 25.  ,   8.  ,   4.32, ...,   0.  ,  58.  ,   1.  ],\n",
       "       [447.  ,   8.  ,   5.2 , ...,   0.  ,  49.  ,   0.  ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#환자의 기록과 수술 결과를 X와 y로 구분하여 저장합니다\n",
    "X=Data_set[:,0:17] #속성 attribute\n",
    "Y=Data_set[:,17] #클래스 class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[293.  ,   1.  ,   3.8 , ...,   1.  ,   0.  ,  62.  ],\n",
       "       [  1.  ,   2.  ,   2.88, ...,   1.  ,   0.  ,  60.  ],\n",
       "       [  8.  ,   2.  ,   3.19, ...,   1.  ,   0.  ,  66.  ],\n",
       "       ...,\n",
       "       [406.  ,   6.  ,   5.36, ...,   0.  ,   0.  ,  62.  ],\n",
       "       [ 25.  ,   8.  ,   4.32, ...,   0.  ,   0.  ,  58.  ],\n",
       "       [447.  ,   8.  ,   5.2 , ...,   0.  ,   0.  ,  49.  ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X #환자 상태 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
       "       0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y #수술 후 생존 결과 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0823 16:34:47.265215  1168 deprecation_wrapper.py:119] From C:\\Users\\Berry\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0823 16:34:47.267216  1168 deprecation_wrapper.py:119] From C:\\Users\\Berry\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0823 16:34:47.289221  1168 deprecation_wrapper.py:119] From C:\\Users\\Berry\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#딥러닝 구조를 결정합니다(모델을 설정하고 실행하는 부분입니다.)\n",
    "model=Sequential()\n",
    "model.add(Dense(30,input_dim=17,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0823 16:34:47.405247  1168 deprecation_wrapper.py:119] From C:\\Users\\Berry\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0823 16:34:47.518272  1168 deprecation_wrapper.py:119] From C:\\Users\\Berry\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0823 16:34:47.574285  1168 deprecation_wrapper.py:119] From C:\\Users\\Berry\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "470/470 [==============================] - 0s 555us/step - loss: 0.6615 - acc: 0.3149\n",
      "Epoch 2/30\n",
      "470/470 [==============================] - 0s 72us/step - loss: 0.1488 - acc: 0.8511\n",
      "Epoch 3/30\n",
      "470/470 [==============================] - 0s 72us/step - loss: 0.1488 - acc: 0.8511\n",
      "Epoch 4/30\n",
      "470/470 [==============================] - 0s 67us/step - loss: 0.1488 - acc: 0.8511\n",
      "Epoch 5/30\n",
      "470/470 [==============================] - 0s 64us/step - loss: 0.1488 - acc: 0.8511\n",
      "Epoch 6/30\n",
      "470/470 [==============================] - 0s 66us/step - loss: 0.1487 - acc: 0.8511\n",
      "Epoch 7/30\n",
      "470/470 [==============================] - 0s 66us/step - loss: 0.1487 - acc: 0.8511\n",
      "Epoch 8/30\n",
      "470/470 [==============================] - 0s 60us/step - loss: 0.1487 - acc: 0.8511\n",
      "Epoch 9/30\n",
      "470/470 [==============================] - 0s 60us/step - loss: 0.1487 - acc: 0.8511\n",
      "Epoch 10/30\n",
      "470/470 [==============================] - 0s 60us/step - loss: 0.1486 - acc: 0.8511\n",
      "Epoch 11/30\n",
      "470/470 [==============================] - 0s 60us/step - loss: 0.1498 - acc: 0.8447\n",
      "Epoch 12/30\n",
      "470/470 [==============================] - 0s 60us/step - loss: 0.1486 - acc: 0.8511\n",
      "Epoch 13/30\n",
      "470/470 [==============================] - 0s 57us/step - loss: 0.1485 - acc: 0.8511\n",
      "Epoch 14/30\n",
      "470/470 [==============================] - 0s 57us/step - loss: 0.1483 - acc: 0.8511\n",
      "Epoch 15/30\n",
      "470/470 [==============================] - 0s 62us/step - loss: 0.1485 - acc: 0.8511\n",
      "Epoch 16/30\n",
      "470/470 [==============================] - 0s 62us/step - loss: 0.1490 - acc: 0.8447\n",
      "Epoch 17/30\n",
      "470/470 [==============================] - 0s 62us/step - loss: 0.1479 - acc: 0.8489\n",
      "Epoch 18/30\n",
      "470/470 [==============================] - 0s 60us/step - loss: 0.1482 - acc: 0.8468\n",
      "Epoch 19/30\n",
      "470/470 [==============================] - 0s 62us/step - loss: 0.1476 - acc: 0.8511\n",
      "Epoch 20/30\n",
      "470/470 [==============================] - 0s 62us/step - loss: 0.1480 - acc: 0.8511\n",
      "Epoch 21/30\n",
      "470/470 [==============================] - 0s 60us/step - loss: 0.1475 - acc: 0.8511\n",
      "Epoch 22/30\n",
      "470/470 [==============================] - 0s 60us/step - loss: 0.1469 - acc: 0.8511\n",
      "Epoch 23/30\n",
      "470/470 [==============================] - 0s 64us/step - loss: 0.1466 - acc: 0.8511\n",
      "Epoch 24/30\n",
      "470/470 [==============================] - 0s 64us/step - loss: 0.1475 - acc: 0.8489\n",
      "Epoch 25/30\n",
      "470/470 [==============================] - 0s 68us/step - loss: 0.1471 - acc: 0.8511\n",
      "Epoch 26/30\n",
      "470/470 [==============================] - 0s 66us/step - loss: 0.1466 - acc: 0.8511\n",
      "Epoch 27/30\n",
      "470/470 [==============================] - 0s 62us/step - loss: 0.1472 - acc: 0.8511\n",
      "Epoch 28/30\n",
      "470/470 [==============================] - 0s 62us/step - loss: 0.1471 - acc: 0.8511\n",
      "Epoch 29/30\n",
      "470/470 [==============================] - 0s 62us/step - loss: 0.1470 - acc: 0.8489\n",
      "Epoch 30/30\n",
      "470/470 [==============================] - 0s 60us/step - loss: 0.1461 - acc: 0.8532\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b8acde7240>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#딥러닝을 실행하다\n",
    "model.compile(loss='mean_squared_error',optimizer='adam',metrics=['accuracy'])\n",
    "model.fit(X,Y,epochs=30,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470/470 [==============================] - 0s 57us/step\n",
      "\n",
      " Accuracy: 0.8511\n"
     ]
    }
   ],
   "source": [
    "#결과를 출력합니다. \n",
    "print(\"\\n Accuracy: %.4f\" %(model.evaluate(X,Y)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. 모델 설계하기 \n",
    "##### # 2장 생존률 예측 설명 \n",
    "\n",
    "딥러닝의 모델을 설정하고 구동하는 부분은 모두 'model'이라는 함수를 선언하며 시작이 됩니다. \n",
    "\n",
    "+ model =Sequential() : 딥러닝의 구조를 짜고 층을 설정하는 부분 \n",
    "+ model.compile() : 모델을 컴퓨터가 알아들을 수 있게끔 컴파일 하는 부분 \n",
    "+ model.fit() : 모델을 실제로 수행하는 부분 \n",
    "\n",
    "## 2. 입력층, 은닉층, 출력층 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=Sequential()\n",
    "# model.add(Dense(30,input_dim=17,activation='relu'))\n",
    "# model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential() 함수를 model로 선언해 놓고 model.add()라는 라인을 추가하면 새로운 층이 만들어진다.\n",
    "model.add()로 시작되는 라인이 두 개가 있으므로 두 개의 층을 가진 모델을 만드는 것이다. \n",
    "맨 마지막 층은 결과를 출력하는 '출력층'이 됩니다. \n",
    "나머지는모두 '은닉층'의 역할을 합니다.\n",
    "따라서 지금 만들어진 이 두 개의 층은 각각 은닉층과 출력층입니다. \n",
    "각각의 층은 Dense라는 함수를 통해 구체적으로 그 구조가 결정됩니다.\n",
    "\n",
    "#### Dense(30,input_dim=17,activation='relu') \n",
    "+ 30 : 이 층에 30개의 노드를 만들겠다.\n",
    "+ input_dim : 첫 번째 은닉층에 input_dim을 적어줌으로써 첫 번째 Dense가 은닉층 + 입력층의 역할을 겸한다. 우리가 다루는 폐암 수술 환자의 생존 여부 데이터에는 17개의 입력 값들이 있습니다. 따라서 데이터에서 17개의 값을 받ㅇ ㅏ은닉층에 30개 노드로 보낸다는 뜻입니다. \n",
    "\n",
    "이제 은닉층의 각 노드는 17개의 입력 값으로부터 임의의 가중치를 가지고 각 노드로 전송되어 활성화 함수를 만납니다. 그리고 활성화 함수를 거친 결과 값이 출력층으로 전달됩니다. \n",
    "+ activation : 우리가 원하는 활성화 함수를 적어주면 됩니다. 앞서 배운 렐루를 사용 \n",
    "\n",
    "#### model.add(Dense(1,activation='sigmoid')) \n",
    "마지막 층이므로 이 층이 곧 출력층이 됩니다. 출력 값을 하나로 정해서 보여 줘야 하므로 출력층의 노드 수는 1개 입니다. \n",
    "그리고 이 노드에서 입력받은 값 역시 활성화 함수를 거쳐 최종 출력 값으로 나와야 합니다. 여기서는 시그모이드를 활성화 함수로 사용했습니다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 컴파일 \n",
    "다음으로 model.compile() 부분입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss='mean_squared_error',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.compile 부분은 앞서 지정한 모델이 효과적으로 구현될 수 있게 여러가지 환경을 설정해 주면서 컴파일하는 부분입니다.\n",
    "+ loss : 먼저 어떤 오차 함수를 사용할지를 정해야 합니다. 여기서는 우리가 2장에서 배운 '평균 제곱 오차 함수(mean_squared_error)를 사용했습니다. \n",
    "그런데 경우에 따라서는 오차 함수를 바꾸어주면 더 좋은 효과를 나타내기도 합니다. \n",
    "오차 함수에는 평균 제곱 오차 계열의 함수 외에도 교차 엔트로피 계열의 함수가 있습니다. 평균 제곱 오차는 수렴하기까지 속도가 많이 걸린다는 단점이 있습니다.\n",
    "교차 엔트로피는 출력 값에 로그를 취해서, 오차가 커지면 수렴 속도가 빨라지고 오차가 작아지면 속도가 감소하게끔 만단 것입니다. \n",
    "교차 엔트로피에 대해서는 다음 절에서 다시 배우겠습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ optimizer: 이어서 최적화를 위한 방법도 간단히 고를 수 있습니다. 앞서 배운 아담(adam)을 사용했습니다. \n",
    "\n",
    "+ metrics : 다음으로 metrics 함수는 모델이 컴파일될 때 모델 수행 결과를 나타내게끔 설정하는 부분입니다. 정확도를 측정하기 위해 사용되는 테스트 샘플을 학습 과정에서 제외시킴으로써 과정합 문제(over fitting,특정 데이터에서는 잘 작동하나 다른 데이터에서는 잘 작동하지 않는 문제)를 방지하는 기능을 담고 있습니다. \n",
    "\n",
    "![nn](10.1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 교차 엔트로피 \n",
    "교차 엔트로피는 주로 분류 문제에서 많이 사용되는데, 특별히 예측 값이 참과 거짓 둘 중 하나인 형식일 때는 binary_crossentropy(이항 교차 엔트로피)를 씁니다. 지금 구하고자 하는 것은 생존(1) 또는 사망(0) 둘 중 하나이므로 binary_crossentropy를 사용할 수 있는 좋은 예라고 할 수 있습니다. 이처럼 케라스에서는 간단한 입력만으로도 다양한 오차 함수를 적용할 수 있는데, 현재 딥러닝과 관련하여 케라스에서 사용되는 대표적인 오차 함수는 다음과 같습니다. \n",
    "\n",
    "![nn](10.2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 모델 실행하기 \n",
    "모델을 정의하고 컴파일하고 나면 이제 실행시킬 차례입니다. 앞서 컴파일 단계에서 정해진 환경을 주어딘 데이터를 불러 실행시킬 때 사용되는 함수가 model.fit() 부분입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(X,Y,epochs=30,batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 부분을 설명하기에 앞서 먼저 용어를 다시 한번 정리해 보겠습니다. \n",
    "![nn](10.3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "폐암 생존 환자 데이터는 총 470명의 환자에게서 17개의 정보를 정리한 것입니다.\n",
    "이때 각 정보를 속성이라고 부릅니다. 그리고 생존 여부를 클래스, 가로 한 줄에 해당하는 각 환자의 정보를 각각 샘플이라고 합니다. \n",
    "주어진 데이터에는 총 470개의 샘플이 각각 17개씩의 속성을 가지고 있는 것입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 프로세스가 모든 샘플에 대해 한 번 실행되는 것을 1 epoch라고 합니다. epochs=1000으로 지정한 것은 각 샘플이 처음부터 끝까지 1000번 재사용될 때까지 실행을 반복하라는 뜻입니다. batch_size=10은 전체 470개의 샘플을 10개씩 끊어서 집어 넣으라는 뜻이 됩니다. batch_size가 너무 크면 학습 속도가 느려지고, 너무 작으면 각 실행 값의 편차가 생겨서 전체 결괏값이 불안정해질 수 있습니다. 자신의 컴퓨터 메모리가 감당할 만큼의 batch_size를 찾아 설정해주는 것이 좋습니다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
