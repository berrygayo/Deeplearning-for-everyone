{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다층 퍼셉트론이 오차 역전파를 만나 신경망이 되었고, 신경망은 XOR 문제를 가볍게 해결했습니다. 따라서 이제 신경망을 차곡차곡 쌓아올리면 마치 사람처럼 생각하고 판단하는 인공지능이 금방이라도 완성될 것 처럼 보였습니다.\n",
    "\n",
    "![nn](9.1.jpg)\n",
    "\n",
    "하지만 기대만큼 결과가 좋지 않았습니다. 그 이유는 무엇이며, 이를 어떻게 해결했는지를 공부해 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기울기 소실 문제와 활성화 함수 \n",
    "\n",
    "오차 역전파는 출력층으로부터 하나씩 앞으로 되돌아가며 각 층의 가중치를 수정하는 방법입니다.\n",
    "가중치를 수정하려면 미분 값, 즉 기울기가 필요하다고 배웠습니다.\n",
    "그런데 층이 늘어나면서 기울기가 중간에 0이 되어버리는 기울기 소실(vanishing gradient) 문제가 발생하기 시작했습니다. \n",
    "\n",
    "![nn](9.2.1.jpg)\n",
    "\n",
    "이는 활성화 함수로 사용된 시그모이드 함수의 특성 때문입니다. \n",
    "아래 그림처럼 시그모이드를 미분하면 최대치가 0.3입니다.\n",
    "1보다 작으므로 계속 곱하다보면 0에 가까워집니다.\n",
    "따라서 층을 거쳐 갈수록 기울기가 사라져 가중치를 수정하기가 어려워지는 것입니다.\n",
    "\n",
    "\n",
    "![nn](9.2.2.jpg)\n",
    "\n",
    "이를 해결하고자 활성화 함수를 시그모이드가 아닌 여러 함수로 대체하기 시작했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nn](9.3.jpg)\n",
    "\n",
    "+ 하이퍼볼릭 탄젠트(tanh) : 시그모이드 함수를 -1에서 1로 확장한 개념 > 미분한 값의 범위가 함께 확장되는 효과 > 하지만 1보다 작은 값이 존재하므로 기울기 소실문제는 미해결 \n",
    "\n",
    "+ 렐루(ReLU) : 시그모이드의 대안으로 떠오르며 현재 가장 많이 사용되는 활성화 함수. x가 0보다 작을 때는 모든 값을 0으로 처리하고, 0보다 큰 값은 x를 그대로 사용하는 방법입니다. x가 0보다 크기만 하면 미분 값이 1이 됩니다 > 여러 은닉층을 거치며 곱해지더라도 맨 처음 층까지 사라지지 않고 남아있을 수 있다. \n",
    "\n",
    "+ 소프트플러스(softplus) : 렐루의 0이 되는 순간을 완화한 소프트 플러스도 개발 중이다. \n",
    "\n",
    "계속해서 더 나은 활성화 함수를 만들기 위한 노력이 이어지고 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 속도와 정확도 문제를 해결하는 고급 경사 하강법 \n",
    "\n",
    "+ 가중치 업데이트 방법 > 경사 하강법 > 고급 경사 하강법 \n",
    "\n",
    "그런데 경사 하강법은 정확하게 가중치를 찾아가지만, 한 번 업데이트 할 때마다 전체 데이터를 미분해야 하므로 계산량이 매우 많다는 단점이 있습니다.\n",
    "이러한 점을 보완한 고급 경사 하강법이 등장하면서 딥러닝의 발전 속도는 더 빨라졌습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 확률적 경사 하강법(SGB)\n",
    "\n",
    "경사 하강법의 불필요하게 많은 계산량은 속도를 느리게 할 뿐 아니라, 최적 해를 찾기 전에 최적화 과정이 멈출 수도 있습니다.\n",
    "확률적 경사 하강법(Stochastic Gradient Descent, SGD)은 이러한 속도의 단점을 보완한 방법입니다. \n",
    "전체 데이터를 사용하는 것이 아니라, 랜덤하게 추출한 일부 데이터를 사용합니다 > 더 빨리 그리고 자주 업데이트 가능 \n",
    "\n",
    "![nn](9.4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모멘텀 \n",
    "\n",
    "모멘텀(momentum)이란 단어는 관성, 탄력, 가속도라는 뜻입니다.\n",
    "모멘텀은 SGD란 말 그대로 경사 하강법에 탄력을 더해 주는 것입니다. 다시 말해서, 경사 하강법과 마찬가지로 매번 기울기를 구하지만,\n",
    "이를 통해 오차를 수정하기 전 바로 앞 수정 값과 방향(+,-)을 참고하여 같은 방향으로 일정한 비율만 수정되게 하는 방법.\n",
    "따라서 수정 방향이 양수(+) 방향으로 한 번, 음수(-) 방향으로 한 번 지그재그로 일어나는 현상이 줄어들고, 이전 이동 값을 고려하여 일정 비율만큼만 다음 값을 결정하므로 관성의 효과를 낼 수 있습니다.\n",
    "\n",
    "![nn](9.5.1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그외 고급 경사 하강법 \n",
    "\n",
    "![nn](9.5.2.jpg)\n",
    "![nn](9.6.jpg)\n",
    "\n",
    "각 방법이 개발된 순서대로 정리하였습니다. 먼저 나온 방법의 단점을 보완하여 다음 방법이 나온 만큼 나중ㅇ 나온 고급 경사 하강법이  좋은 성과를 보입니다. \n",
    "\n",
    "맨 마지막에 기재된 아담(Adam)은 현재 가장 많이 사용되는 고급 경사 하강법입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
