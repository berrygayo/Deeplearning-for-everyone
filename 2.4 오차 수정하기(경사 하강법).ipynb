{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기울기 a를 너무 크게 잡아도 오차는 커지고 작게 잡아도 커집니다. 기울기와 오차 사이에는 상관관계가 있음을 알 수 있습니다. \n",
    "오차가 가장 작은 지점은 가장 아래쪽의 볼록한 부분에 이르렀을 때 입니다. \n",
    "가장 오차가 작은 부분을 찾기위해 미분 기울기를 이용하는 방법이 경사 하강법(gradient decent) 입니다. \n",
    "![nn](learningrate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사 하강법의 개요 \n",
    "\n",
    "1. a1에서 미분을 구한다.\n",
    "2. 구해진 기울기의 반대 방향(기울기가 +면 음의 방향, -면 양의 방향)으로 얼마간 이동시킨 a2에서 미분을 구한다.\n",
    "3. a3에서 미분을 구한다\n",
    "4. 3의 값이 0이 아니면 위 과정을 반복한다.\n",
    "\n",
    "그러면 그림 4-4처럼 기울기가 0인 한 점(m)으로 수렴합니다. \n",
    "경사 하강법은 이렇게 반복적으로 기울기 a를 변화시켜서 m의 값을 찾아내는 방법을 말합니다. \n",
    "\n",
    "![nn](4.2.4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습률 \n",
    "여기서 우리는 학습률(learning rate)이라는 개념을 알 수 있습니다. 기울기의 부호를 바꿔 이동시킬 때 적절한 거리를 찾지 못해 너무 멀ㄹ ㅣ이동시키면 a 값이 한 점으로 모이지 않고 그림 4-5 처럼 위로 치솟아 버립니다. \n",
    "따라서 어느만큼 이동시킬지를 신중히 결정해야 하는데, 이때 이동 거리를 정해주는 것이 바로 학습률입니다. 딥러닝에서 학습률의 값ㅇ르 적절히 바꾸면서 최적의 학습률을 찾는 것은 중요한 최적화 과정 중 하나입니다. \n",
    "(참고로 케라스는 학습률을 자동으로 조절해 줍니다. 하지만 딥러닝을 배우려면 학습률의 개념을 알아두는 것이 중요합니다. )\n",
    "\n",
    "![nn](4.2.5.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코딩으로 확인하는 경사 하강법\n",
    "이제 코딩을 통해 경사 하강법을 실제로 적용하여 a와 b의 값을 구해 보겠습니다. 여기서는 텐서플로 라이브러리를 불러와 사용해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[[2,81],[4,93],[6,91],[8,97]]\n",
    "x_data=[x_row[0] for x_row in data]\n",
    "y_data=[y_row[1] for y_row in data]\n",
    "\n",
    "learning_rate=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기울기 a와 y절편 b의 값을 임의로 정하겠습니다.\n",
    "다만, 기울기가 너무 커지거나 작아지면 실행 시간이 불필요하게 늘어나므로 기울기는 0~10 사이에서, y절편은 0~100 사이에서 임의의 값을 얻게끔 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.Variable(tf.random_uniform([1],0,10,dtype=tf.float64,seed=0)) #0~10사이의 수를 [1]개 만들어라! \n",
    "b=tf.Variable(tf.random_uniform([1],0,100,dtype=tf.float64,seed=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=a*x_data+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "평균 제곱근 오차 식 구현\n",
    "- 앞서 배운 평균 제곱근 오차는 텐서플로에 내재된 다음 각 함수를 통해 구현할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse=tf.sqrt(tf.reduce_mean(tf.square(y-y_data)))\n",
    "#tf.sqrt(x):x의 제곱근을 계산 \n",
    "#tf.reduce_mean(x) : x의 평균을 계산 \n",
    "#tf.square(x) : x의 제곱을 계산 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사 하강법을 실행.\n",
    "GradientDescentOptimizer() 함수를 이용해 다음과 같이 경사 하강법의 결과를 dradient_decent에 할당."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_decent=tf.train.GradientDescentOptimizer(learning_rate).minimize(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 지정한 learning_rate와 평균 제곱근 오차를 통해 구한 rmse가 포함된 것을 볼 수 있습니다. 이제 텐서플로를 실행시키고, 결괏값을 출력하는 부분을 다음과 같이 작성합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, RMSE=30.2139, 기울기 a=7.5235, y절편 b=80.5984\n",
      "Epoch:100, RMSE=2.8860, 기울기 a=2.2299, y절편 b=79.4181\n",
      "Epoch:200, RMSE=2.8826, 기울기 a=2.2601, y절편 b=79.2379\n",
      "Epoch:300, RMSE=2.8815, 기울기 a=2.2773, y절편 b=79.1353\n",
      "Epoch:400, RMSE=2.8811, 기울기 a=2.2871, y절편 b=79.0770\n",
      "Epoch:500, RMSE=2.8810, 기울기 a=2.2927, y절편 b=79.0438\n",
      "Epoch:600, RMSE=2.8810, 기울기 a=2.2958, y절편 b=79.0249\n",
      "Epoch:700, RMSE=2.8810, 기울기 a=2.2976, y절편 b=79.0142\n",
      "Epoch:800, RMSE=2.8810, 기울기 a=2.2987, y절편 b=79.0081\n",
      "Epoch:900, RMSE=2.8810, 기울기 a=2.2992, y절편 b=79.0046\n",
      "Epoch:1000, RMSE=2.8810, 기울기 a=2.2996, y절편 b=79.0026\n",
      "Epoch:1100, RMSE=2.8810, 기울기 a=2.2998, y절편 b=79.0015\n",
      "Epoch:1200, RMSE=2.8810, 기울기 a=2.2999, y절편 b=79.0008\n",
      "Epoch:1300, RMSE=2.8810, 기울기 a=2.2999, y절편 b=79.0005\n",
      "Epoch:1400, RMSE=2.8810, 기울기 a=2.3000, y절편 b=79.0003\n",
      "Epoch:1500, RMSE=2.8810, 기울기 a=2.3000, y절편 b=79.0002\n",
      "Epoch:1600, RMSE=2.8810, 기울기 a=2.3000, y절편 b=79.0001\n",
      "Epoch:1700, RMSE=2.8810, 기울기 a=2.3000, y절편 b=79.0001\n",
      "Epoch:1800, RMSE=2.8810, 기울기 a=2.3000, y절편 b=79.0000\n",
      "Epoch:1900, RMSE=2.8810, 기울기 a=2.3000, y절편 b=79.0000\n",
      "Epoch:2000, RMSE=2.8810, 기울기 a=2.3000, y절편 b=79.0000\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    #변수 초기화 \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #2001번 실행(0번째를 포함하므로 )\n",
    "    for step in range(2001):\n",
    "        sess.run(gradient_decent)\n",
    "        #100번마다 결과출력 \n",
    "        if step % 100==0 : \n",
    "            print(\"Epoch:%.f, RMSE=%.04f, 기울기 a=%.4f, y절편 b=%.4f\" %(step, sess.run(rmse),sess.run(a),sess.run(b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서플로는 session 함수를 이용해 구동ㅇ ㅔ필요한 리소스를 컴퓨터에 할당하고 이를 실행시킬 준비를 합니다.\n",
    "Session을 통해 구현될 함수를 텐서플로에서는 '그래프'라고 부르며, Session이 할당되면 session.run('그래프명')의 형식으로 해당 함수를 구동시킵니다. \n",
    "global_variables_initializer()는 변수를 초기화하는 함수입니다. \n",
    "앞서 만든 gradient_decent를 총 필요한 수만큼 반복하여 실행합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기울기가 2.3 y절편이 79에 수렴하는 고자ㅓㅇㅇ르 볼 수 있다.\n",
    "이는 우리가 앞서 최소 제곱법을 통해 미리 확인한 정답 값과 같습니다.\n",
    "이렇게 하면 최소 제곱법을 쓰지 않고 평균 제곱근 오차를 구하고,\n",
    "경사 하강법을 통해 기울기 a와 y절편 b 값을 구할 수 있습니다.\n",
    "이와 똑같은 방식을 x가 여러 개인 다중 선형 회귀에서도 사용합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다중 선형 회귀란? \n",
    "성적에 단순히 공부 시간만 영향을 끼치는 것이 아니라 다른 변수 또한 영향을 끼칠 수 있다.\n",
    "정보를 추가하여 새로운 예측값을 구하려면 변수의 개수를 늘려 '다중 선형 회귀'를 만들어 주어야 한다.\n",
    "예를 들어, 일주일 동안 받는 과외 수업 횟수를 조사해서 이를 기록해 보았다고 하자.\n",
    "\n",
    "그럼 지금부터 두 개의 독립 변수 x1(공부 시간), x2(과외 수업 횟수)가 생긴 것이다. \n",
    "y=a1x1+a2x2+b \n",
    "\n",
    "그러면 두 기울기 a1과 a2는 각각 어떻게 구할 수 있을까요? \n",
    "앞에서 배운 경사 하강법을 그대로 적용하면 됩니다. \n",
    "바로 코딩을 통해 확인해 보겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#x1, x2, y의 데이터 값 \n",
    "data=[[2,0,81],[4,4,93],[6,2,91],[8,3,97]] #공부시간, 과외 수업 횟수, 성적 \n",
    "\n",
    "x1=[x_row1[0] for x_row1 in data]\n",
    "x2=[x_row2[1] for x_row2 in data] #새로 추가되는 값 \n",
    "y_data=[y_row[2] for y_row in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 앞서 기울기의 값을 구하는 방식 그대로 또 하나의 기울기 a2를 구합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1=tf.Variable(tf.random_uniform([1],0,10,dtype=tf.float64,seed=0))\n",
    "a2=tf.Variable(tf.random_uniform([1],0,10,dtype=tf.float64,seed=0)) #새로 추가되는 값 \n",
    "b=tf.Variable(tf.random_uniform([1],0,100,dtype=tf.float64,seed=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "새로운 방정식 y=a1x1+a2x2+b에 맞춰 다음과 같이 식을 세웁니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=a1*x1+a2*x2+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "나머지 라인은 앞서 배운 선형 회귀와 같습니다. 결과를 출력하는 부분만 기울기가 두 개 나올 수 있게 수정합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#텐서플로 rmse 함수 \n",
    "rmse=tf.sqrt(tf.reduce_mean(tf.square(y-y_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습률 값 \n",
    "learning_rate=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rmse 값을 최소로 하는 값 찾기 \n",
    "gradient_decent=tf.train.GradientDescentOptimizer(learning_rate).minimize(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, RMSE=49.1842, 기울기 a1=7.5270, 기울기 a2=7.8160, y절편 b=80.5980\n",
      "Epoch:100, RMSE=1.8368, 기울기 a1=1.1306, 기울기 a2=2.1316, y절편 b=78.5119\n",
      "Epoch:200, RMSE=1.8370, 기울기 a1=1.1879, 기울기 a2=2.1487, y절편 b=78.1057\n",
      "Epoch:300, RMSE=1.8370, 기울기 a1=1.2122, 기울기 a2=2.1571, y절편 b=77.9352\n",
      "Epoch:400, RMSE=1.8370, 기울기 a1=1.2226, 기울기 a2=2.1607, y절편 b=77.8636\n",
      "Epoch:500, RMSE=1.8370, 기울기 a1=1.2269, 기울기 a2=2.1622, y절편 b=77.8335\n",
      "Epoch:600, RMSE=1.8370, 기울기 a1=1.2288, 기울기 a2=2.1628, y절편 b=77.8208\n",
      "Epoch:700, RMSE=1.8370, 기울기 a1=1.2295, 기울기 a2=2.1631, y절편 b=77.8155\n",
      "Epoch:800, RMSE=1.8370, 기울기 a1=1.2299, 기울기 a2=2.1632, y절편 b=77.8133\n",
      "Epoch:900, RMSE=1.8370, 기울기 a1=1.2300, 기울기 a2=2.1632, y절편 b=77.8124\n",
      "Epoch:1000, RMSE=1.8370, 기울기 a1=1.2301, 기울기 a2=2.1633, y절편 b=77.8120\n",
      "Epoch:1100, RMSE=1.8370, 기울기 a1=1.2301, 기울기 a2=2.1633, y절편 b=77.8118\n",
      "Epoch:1200, RMSE=1.8370, 기울기 a1=1.2301, 기울기 a2=2.1633, y절편 b=77.8117\n",
      "Epoch:1300, RMSE=1.8370, 기울기 a1=1.2301, 기울기 a2=2.1633, y절편 b=77.8117\n",
      "Epoch:1400, RMSE=1.8370, 기울기 a1=1.2301, 기울기 a2=2.1633, y절편 b=77.8117\n",
      "Epoch:1500, RMSE=1.8370, 기울기 a1=1.2301, 기울기 a2=2.1633, y절편 b=77.8117\n",
      "Epoch:1600, RMSE=1.8370, 기울기 a1=1.2301, 기울기 a2=2.1633, y절편 b=77.8117\n",
      "Epoch:1700, RMSE=1.8370, 기울기 a1=1.2301, 기울기 a2=2.1633, y절편 b=77.8117\n",
      "Epoch:1800, RMSE=1.8370, 기울기 a1=1.2301, 기울기 a2=2.1633, y절편 b=77.8117\n",
      "Epoch:1900, RMSE=1.8370, 기울기 a1=1.2301, 기울기 a2=2.1633, y절편 b=77.8117\n",
      "Epoch:2000, RMSE=1.8370, 기울기 a1=1.2301, 기울기 a2=2.1633, y절편 b=77.8117\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    #변수 초기화 \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #2001번 실행(0번째를 포함하므로 )\n",
    "    for step in range(2001):\n",
    "        sess.run(gradient_decent)\n",
    "        #100번마다 결과출력 \n",
    "        if step % 100==0 : \n",
    "            print(\"Epoch:%.f, RMSE=%.04f, 기울기 a1=%.4f, 기울기 a2=%.4f, y절편 b=%.4f\" %(step, sess.run(rmse),sess.run(a1),sess.run(a2),sess.run(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
